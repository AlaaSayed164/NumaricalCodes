# NumaricalCodes
Numaical optimization 


[1.implement the gradient descent for linear regressionwith one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%201%20GD%20Implementation%20for%20LR%20.ipynb)

[2.implement the gradient descent variants (Batch/Mini-Batch/Stochastic) for linear regression with one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%202%20GD%20Variants%20Batch%20-%20Mini-Batch%20-%20Stochastic.ipynb)

[3.implement the accelerated gradient descent methods (Momentum and NAG) for linear regression with one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%203%20Momentum%20-%20NAG%20(1).ipynb)

[4.implement the accelerated gradient descent methods with adaptive learning rate (Adagrad, RMSProp, and Adam) for linear regression with one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%204%20Adagrad-RMSProp-Adam%20(1).ipynb)

