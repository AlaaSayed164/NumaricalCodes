# Numerical Codes
Machine learning optimisation:
is the process of iteratively improving the accuracy of a machine learning model, lowering the degree of error. Machine learning models learn to generalise and make predictions about new live data based on insight learned from training data. This works by approximating the underlying function or relationship between input and output data. A major goal of training a machine learning algorithm is to minimise the degree of error between the predicted output and the true output. 

Optimisation is measured through a loss or cost function, which is typically a way of defining the difference between the predicted and actual value of data. Machine learning models aim to minimise this loss function, or lower the gap between prediction and reality of output data. Iterative optimisation will mean that the machine learning model becomes more accurate at predicting an outcome or classifying data.  


Numerical  optimization 

[1.implement the gradient descent for linear regressionwith one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%201%20GD%20Implementation%20for%20LR%20.ipynb)

[2.implement the gradient descent variants (Batch/Mini-Batch/Stochastic) for linear regression with one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%202%20GD%20Variants%20Batch%20-%20Mini-Batch%20-%20Stochastic.ipynb)

[3.implement the accelerated gradient descent methods (Momentum and NAG) for linear regression with one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%203%20Momentum%20-%20NAG%20(1).ipynb)

[4.implement the accelerated gradient descent methods with adaptive learning rate (Adagrad, RMSProp, and Adam) for linear regression with one variable 
](https://github.com/AlaaSayed164/NumaricalCodes/blob/main/Practical%20Session%204%20Adagrad-RMSProp-Adam%20(1).ipynb)

